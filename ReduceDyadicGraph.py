"""
Reads a file that was generated by GenerateDyadicGraph.py, and returns a reduced form, to account for aggregation.
So far only works with simple aggregation (multiplicities = 1), and no fractional/weighted values
"""

import os

import pandas as pd
import numpy as np


def read(fn="./data/dyadic_edges.csv", sep=" "):
    return pd.read_csv(fn, sep=sep, header=None, skiprows=0).values


def write(out, fn="./data/dyadic_edges.csv", sep="\t"):
        np.savetxt(fn, out, delimiter=sep, fmt="%i")


def processInBatch(outFilename, separatorInputFile, separatorOutputFile, inFilename="./data/dyadic_edges.csv"):
    """
    Does the same thing, but iteratively, since the full edge list is 47 GB...
    :param outFilename:
    :param separatorInputFile:
    :param separatorOutputFile:
    :param fn: Name of the file to be read.
    :return: (None)
    """

    uniqueEdgeList = {}
    with open(inFilename, "r") as f:
        for line in f:
            currentEdge = uniqueEdgeList.get(line.strip("\n"), 0)
            uniqueEdgeList[line.strip("\n")] = currentEdge+1

    with open(outFilename, "a") as f:
        for edge, weight in uniqueEdgeList.items():
            weightedEdge = separatorOutputFile.join(edge.split(separatorInputFile)) + separatorOutputFile + str(weight) + "\n"
            f.write(weightedEdge)


if __name__ == "__main__":
    # TODO: DOES NOT WORK FOR THE FULL GRAPH!
    # folder = os.path.dirname(os.path.abspath(__file__))
    # fn = os.path.join(folder, "data/dyadic_full_edges.csv")
    # data = read(fn=fn)
    #
    # unique, counts = np.unique(data, axis=0, return_counts=True)
    #
    # out = np.concatenate([unique, np.expand_dims(counts, axis=1)], axis=1)
    # fn_out = os.path.join(folder, "data/test.csv")
    # write(out, fn_out)

    # touch file at the beginning to overwrite, since we are later appending
    # https://stackoverflow.com/questions/2769061/how-to-erase-the-file-contents-of-text-file-in-python
    filename = "./data/hyperedges_subset_reduced.csv"
    open(filename, "w").close()
    processInBatch(outFilename=filename, separatorInputFile=" ", separatorOutputFile="\t",
                   inFilename="./data/hyperedges_subset.csv")


